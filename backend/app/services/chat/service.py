from __future__ import annotations

import asyncio
import json
from typing import List, Dict

from ask_forge.backend.app.core.app_state import AppState
from ask_forge.backend.app.repositories.vectorstore import ChromaRepo
from ask_forge.backend.app.services.chat.schemas import ChatBody
from ask_forge.backend.app.services.chat.pipeline import (
    prepare_contexts_for_response,
    build_history_context,
    build_system_memory_block, build_chat_prompt_from_template, stream_answer_llm,
)
# from ask_forge.backend.app.services.queue.redis_queue import BackgroundQueueUsingRedis
from ask_forge.backend.app.services.chat_history.summary import generate_session_summary

from fastapi.responses import StreamingResponse
import logging
logger = logging.getLogger(__name__)

def _sse(payload: dict | str, event: str | None = None) -> str:
    """SSE format: data: {json}\n\n"""
    if isinstance(payload, dict):
        data = json.dumps(payload, ensure_ascii=False)
    else:
        data = payload

    if event:
        return f"event: {event}\ndata: {data}\n\n"
    return f"data: {data}\n\n"

class ChatService:
    def __init__(self,app_state : AppState, repo: ChromaRepo):
        self.repo = repo
        self.app_state = app_state
        self.question_generator_service = app_state.llm_registry.get("question_generator_service") # llm_registered ·ªü app_state
        self.chat_history = app_state.history_repo

    def _retrieve(self, *, index_name: str, query_text: str, n_results: int = 3, min_rel: float = 0.5) -> List[Dict]:
        return self.repo.get_context_for_chat(
            index_name=index_name,
            query_text=query_text,
            n_results=n_results,
            min_relevance=min_rel,
        )

    async def chat_stream_sse(self, body: ChatBody):
        """Generator tr·∫£ SSE chunks theo chu·∫©n"""
        async def event_gen():
            try:
                # ===== 1. Retrieve contexts (non-blocking) =====
                contexts = await asyncio.to_thread(
                    self._retrieve,
                    index_name=body.index_name,
                    query_text=body.query_text,
                    n_results=body.n_results,
                    min_rel=body.min_rel,
                )


                logger.info(f"üìö Retrieved {len(contexts)} contexts for streaming")
                # Optional ping connection

                # yield _sse({
                #     "type": "ping",
                #     "content": "start"
                # })

                # ===== 2. Build prompt =====
                prompt = build_chat_prompt_from_template(
                    question=body.query_text,
                    contexts=contexts,
                    lang=body.lang
                )

                # ===== 3. Stream answer tokens =====
                async for chunk in stream_answer_llm(
                        prompt=prompt,
                        app_state=self.app_state,
                        task="chat"
                ):
                    if chunk:  # Skip empty chunks
                        yield _sse({
                            "type": "token",
                            "content": chunk,
                        })

                # ===== 4. Send contexts (after answer complete) =====
                yield _sse({
                    "type": "contexts",
                    "data": [
                        {
                            "source": c.get("source"),
                            "page": c.get("page"),
                            "preview": c.get("text", "")[:200],
                            "score": c.get("score")
                        }
                        for c in contexts
                    ]
                })

                # ===== 5. Trigger QG background job =====
                try:
                    job_id = await self.app_state.bq.enqueue_qg(
                        seed_question=body.query_text,
                        contexts=contexts,
                        lang=body.lang,
                        session_id=getattr(body, "session_id", "default"),
                        app_state=self.app_state,
                    )
                    logger.info(job_id)
                    # Yield for client to know where the job located (job_id), then the client need to call an API with the job_id
                    # to get the question generate result
                    yield _sse({
                        "type": "qg_job",
                        "job_id": job_id,
                        "poll_url": f"/api/chat/qg/{job_id}"
                    })
                except Exception as e:
                    logger.warning(f"QG job enqueue failed: {e}")
                    # Kh√¥ng crash stream n·∫øu QG fail

            except Exception as e:
                logger.exception("Streaming error")
                yield _sse({
                    "type": "error",
                    "content": str(e)
                })
            finally:
                yield _sse("[DONE]")

        # ==== HTTP response (b·∫Øt bu·ªôc cho SSE) ====
        headers = {
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no", # tr√°nh Nginx/nginx-ingress buffer
        }
        return StreamingResponse(event_gen(), media_type="text/event-stream", headers=headers)
        # StreamingResponse Receive AsyncIterable object to return streaming response.

    async def _summarize_learning_flow(self, sess) -> str:
        """
        G·ªçi LLM t·∫°o t√≥m t·∫Øt l≈©y ti·∫øn:
        - M·ª•c ti√™u c·ªßa H·ªçc Sinh ƒëang theo ƒëu·ªïi l√† g√¨
        - C√°c kh√°i ni·ªám ƒë√£ cover
        - L·ªó h·ªïng/hi·ªÉu sai
        - G·ª£i √Ω b∆∞·ªõc k·∫ø ti·∫øp
        """
        # TODO: T√¥i mu·ªën sau n√†y ta s·∫Ω ƒëi s√¢u v√†o flow summarize, ph·∫ßn summarize n√†y s·∫Ω ph·∫£n √°nh ki·∫øn th·ª©c hi·ªán t·∫°i c·ªßa h·ªçc sinh. Summarize c√≥ th·ªÉ l√† review l·∫°i ch·∫•t l∆∞·ª£ng ƒë·∫∑t c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ xem x√©t g·ª£i √Ω cho ng∆∞·ªùi d√πng nh·ªØng c√°i c·∫ßn thi·∫øt.
        # L·∫•y m·ªôt ƒëo·∫°n nh·ªè l·ªãch s·ª≠ + summary c≈©
        history_block = build_history_context(sess.recent_pairs(sess.last_k + 3))
        # B·∫°n c√≥ th·ªÉ d√πng c√πng `generate_answer_non_stream` v·ªõi 0 contexts, ho·∫∑c t√°ch ra h√†m call LLM ƒë∆°n gi·∫£n
        try:
            summary_text, _ = generate_session_summary(app_state=self.app_state, history_block=history_block)
            return summary_text.strip()
        except Exception as e:
            return ""