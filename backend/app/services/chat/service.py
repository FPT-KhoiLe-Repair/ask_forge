from __future__ import annotations

import asyncio
import json
from typing import List, Dict

from ask_forge.backend.app.core.app_state import AppState
from ask_forge.backend.app.services.qg.service import QGService
from ask_forge.backend.app.repositories.vectorstore import ChromaRepo
from ask_forge.backend.app.services.chat.schemas import ChatBody, ChatResponse, ContextChunk, ChatTurn
from ask_forge.backend.app.services.chat.pipeline import (
    generate_answer_non_stream,
    generate_answer_stream,
    prepare_contexts_for_response,
    build_history_context,
    build_system_memory_block, build_chat_prompt_from_template, stream_answer_llm
)
from ask_forge.backend.app.services.queue.redis_queue import BackgroundQueue
from ask_forge.backend.app.services.chat_history.summary import generate_session_summary

import hashlib
import secrets

import logging
logger = logging.getLogger(__name__)

class ChatService:
    def __init__(self,app_state : AppState, repo: ChromaRepo):
        self.repo = repo
        self.app_state = app_state
        self.question_generator_service = app_state.llm_registry.get("question_generator_service") # llm_registered ·ªü app_state
        self.chat_history = app_state.history_repo
        self.bq_queue = BackgroundQueue()

    def _retrieve(self, *, index_name: str, query_text: str, n_results: int = 3, min_rel: float = 0.5) -> List[Dict]:
        return self.repo.get_context_for_chat(
            index_name=index_name,
            query_text=query_text,
            n_results=n_results,
            min_relevance=min_rel,
        )

    async def chat_stream_sse(self, body: ChatBody):
        """Generator tr·∫£ SSE chunks"""

        # ===== 1. Retrieve contexts (non-blocking) =====
        contexts = await asyncio.to_thread(
            self._retrieve,
            index_name=body.index_name,
            query_text=body.query_text,
            n_results=body.n_results,
            min_rel=body.min_rel,
        )

        logger.info(f"üìö Retrieved {len(contexts)} contexts for streaming")

        # ===== 2. Build prompt =====
        prompt = build_chat_prompt_from_template(
            question=body.query_text,
            contexts=contexts,
            lang=body.lang
        )

        # ===== 3. Stream answer tokens =====
        try:
            async for chunk in stream_answer_llm(
                    prompt=prompt,
                    app_state=self.app_state,
                    task="chat"
            ):
                if chunk:  # Skip empty chunks
                    yield json.dumps({
                        "type": "token",
                        "content": chunk
                    })
        except Exception as e:
            logger.exception("Streaming error")
            yield json.dumps({
                "type": "error",
                "content": str(e)
            })
            return

        # ===== 4. Send contexts (after answer complete) =====
        yield json.dumps({
            "type": "contexts",
            "data": [
                {
                    "source": c.get("source"),
                    "page": c.get("page"),
                    "preview": c.get("text", "")[:200],
                    "score": c.get("score")
                }
                for c in contexts
            ]
        })

        # ===== 5. Trigger QG background job =====
        try:
            job_id = await self.bq_queue.enqueue_qg(
                seed_question=body.query_text,
                contexts=contexts,
                lang=body.lang,
                session_id=getattr(body, "session_id", "default"),
            )

            yield json.dumps({
                "type": "qg_job",
                "job_id": job_id,
                "poll_url": f"/api/chat/qg/{job_id}"
            })
        except Exception as e:
            logger.warning(f"QG job enqueue failed: {e}")
            # Kh√¥ng crash stream n·∫øu QG fail

    async def chat_with_followup_pipeline(self, body: ChatBody):
        """
        Chat system logic: retrieve, answer, generate follow-ups.
        """
        # Generate a random salt
        salt = secrets.token_bytes(16)

        # Data to be hashed (can be anything)
        data = b"my secret message"

        # Combine data and salt, then hash
        hashed_data = hashlib.sha256(salt + data).hexdigest()

        session_id = getattr(body, "session_id", hashed_data) #TODO: trong t∆∞∆°ng lai c√≥ th·ªÉ d√πng chat_session_id ƒë·ªÉ l∆∞u nhi·ªÅu chat_session
        user_id = getattr(body, "user_id", hashed_data) #TODO: t∆∞∆°ng t·ª±, c√°i n√†y ƒë·ªÉ d√†nh cho t∆∞∆°ng lai, c√≤n c√°i n√†y d√πng c√°c default option cho d·ªÖ

        current_session = self.chat_history.get_or_create(session_id=session_id, user_id=user_id)


        logger.info(f"üó£Ô∏è Chat request: {body.query_text} | index={body.index_name}")

        # ---- Step 0: Build memory blocks ----
        history_block = build_history_context(current_session.recent_pairs(current_session.last_k))
        summary_block = build_system_memory_block(current_session.rolling_summary)

        # ---- Step 1: Retrieve Context ----
        contexts = self._retrieve(
            index_name=body.index_name,
            query_text=body.query_text,
            n_results=body.n_results,
            min_rel=body.min_rel,
        )
        logger.info(f"üìö Retrieved {len(contexts)} context chunks")

        # ---- Step 2: Build prompts & Generate Answer ----
        try:
            # TODO: Important! Kh√¥ng d√πng chat/pipline n·ªØa m√† t√≠ch h·ª£p th·∫≥ng s·ª≠ d·ª•ng LLMProvider, l·∫•y GeminiAdapter lu√¥n, ho·∫∑c n·∫øu c·∫ßn g·ªçi v√†o pipeline th√¨ ph·∫£i g·ªçi LLMProvider trong pipeline.
            answer_text, model_name = await generate_answer_non_stream(
                question=body.query_text,
                contexts=contexts,
                lang=body.lang,
                app_state=self.app_state,
                history_block=history_block,
                summary_block=summary_block
            )
        except Exception as e:
            logger.exception(e)
            answer_text = f"Xin l·ªói, c√≥ l·ªói khi truy v·∫´n m√¥ h√¨nh:{e}"
            model_name = ""

        # ---- Step 3: Generate follow-up questions ----
        if self.question_generator_service: # Xem generate ·ªü question_generator.py
            followup_questions = await self.question_generator_service.generate(
                prompt=body.query_text,
                contexts=contexts,
                n=5,
                lang=body.lang,
                history_block=history_block,
                summary_block=summary_block
            )
            seed_question = body.query_text  # Keep original question
        else:
            followup_questions = []
            seed_question = body.query_text

        # ---- Step 4: Append USER turn to history (question) ----
        self.chat_history.append(
            session_id=session_id,
            chat_turn=ChatTurn(
                role="user",
                question=seed_question,
                index_name=body.index_name,
                contexts=None
        ))

        # ---- Step 5: Append ASSISTANT turn (answer) ----
        self.chat_history.append(
            session_id=session_id,
            chat_turn=ChatTurn(
                role="assistant",
                answer_text=answer_text,
                model_name=model_name,
                index_name=body.index_name,
                contexts=[ContextChunk(
                    source=c.get("source"),
                    page=c.get("page"),
                    chunk_id=c.get("chunk_id"),
                    preview=c.get("text","")[:240],
                    text=c.get("text",""),
                    score=c.get("score"),
                ) for c in contexts],
            )
        )

        # ---- (Optional) Update rolling summary m·ªói N l∆∞·ª£t ----
        try:
            if len(current_session.chat_turn) % 6 ==0:
                # T·∫°o prompt t√≥m t·∫Øt l≈©y ti·∫øn t·ª´ history g·∫ßn ƒë√¢y + summary c≈©
                new_summary = await self._summarize_learning_flow(current_session)
                if new_summary:
                    self.chat_history.set_summary(
                        session_id=session_id,
                        new_summary=new_summary
                    )
        except Exception as e:
            logger.exception(f"Rolling summary update failed: {e}")

        # ---- Step 6: Merge Response ---
        contexts_serialized = [
            ContextChunk(
                source=c.get("source"),
                page=c.get("page"),
                chunk_id=c.get("chunk_id"),
                preview=c.get("text", "")[:240],
                text=c.get("text",""),
                score=c.get("score"),
            )
            for c in contexts
        ]
        results = ChatResponse(
            ok=True,
            answer=answer_text or "Xin l·ªói, m√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi ph√π h·ª£p t·ª´ context hi·ªán c√≥.",
            contexts=contexts_serialized,
            followup_questions=followup_questions,
            model_name=model_name,
        )
        logger.info(f"‚úÖ Chat complete | model={model_name} | followups={len(followup_questions)}")
        return results

    async def _summarize_learning_flow(self, sess) -> str:
        """
        G·ªçi LLM t·∫°o t√≥m t·∫Øt l≈©y ti·∫øn:
        - M·ª•c ti√™u c·ªßa H·ªçc Sinh ƒëang theo ƒëu·ªïi l√† g√¨
        - C√°c kh√°i ni·ªám ƒë√£ cover
        - L·ªó h·ªïng/hi·ªÉu sai
        - G·ª£i √Ω b∆∞·ªõc k·∫ø ti·∫øp
        """
        # TODO: T√¥i mu·ªën sau n√†y ta s·∫Ω ƒëi s√¢u v√†o flow summarize, ph·∫ßn summarize n√†y s·∫Ω ph·∫£n √°nh ki·∫øn th·ª©c hi·ªán t·∫°i c·ªßa h·ªçc sinh. Summarize c√≥ th·ªÉ l√† review l·∫°i ch·∫•t l∆∞·ª£ng ƒë·∫∑t c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ xem x√©t g·ª£i √Ω cho ng∆∞·ªùi d√πng nh·ªØng c√°i c·∫ßn thi·∫øt.
        # L·∫•y m·ªôt ƒëo·∫°n nh·ªè l·ªãch s·ª≠ + summary c≈©
        history_block = build_history_context(sess.recent_pairs(sess.last_k + 3))
        # B·∫°n c√≥ th·ªÉ d√πng c√πng `generate_answer_non_stream` v·ªõi 0 contexts, ho·∫∑c t√°ch ra h√†m call LLM ƒë∆°n gi·∫£n
        try:
            summary_text, _ = generate_session_summary(app_state=self.app_state, history_block=history_block)
            return summary_text.strip()
        except Exception as e:
            return ""

    async def chat_once(self, body: ChatBody):
        contexts = self._retrieve(
            index_name=body.index_name,
            query_text=body.query_text,
            n_results=body.n_results,
            min_rel=body.min_rel,
        )
        answer_text, model_name = generate_answer_non_stream(
            question=body.query_text,
            contexts=contexts,
            lang=body.lang,
            app_state=self.app_state,
        )
        return ChatResponse(
            ok=True,
            answer=answer_text or "Xin l·ªói, m√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi ph√π h·ª£p t·ª´ context hi·ªán c√≥.",
            contexts=[ContextChunk(**c,
                                   preview=c.get("text", "")[:240])
                      for c in prepare_contexts_for_response(contexts)],
            followup_questions=[],
            model_name=model_name,
        )

    def chat_stream(self, body: ChatBody):
        contexts = self._retrieve(
            index_name=body.index_name,
            query_text=body.query_text,
            n_results=body.n_results,
            min_rel=body.min_rel,
        )
        return generate_answer_stream(
            question=body.query_text,
            contexts=contexts,
            lang=body.lang,
            app_state=self.app_state,
        )